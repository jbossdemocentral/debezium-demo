// versions
:debezium: 1.1
:streams: 1.5
:camel-kafka-connectors: 0.4.0

// URLs
//:fuse-documentation-url: https://access.redhat.com/documentation/en-us/red_hat_fuse/{fuse-version}/
:openshift-console-url: {openshift-host}/topology/ns/debezium-complete-demo/graph

// attributes
:title: Data Replication
:standard-fail-text: Verify that you followed all the steps. If you continue to have issues, contact your administrator.

// id syntax is used here for the custom IDs
[id='debezium-complete-demo']
= {title}

// Description text for the Solution Pattern.
In this tutorial you will use the MS SQL Server Debezium connector to emit events from a legacy application to replicate _orders_ data to Elasticsearch for future indexing.

// Additional introduction content.
In the first part of this lab you will use the already enabled Change Data Capture (CDC) for a legacy application that loads the customer orders from a comma separated value (CSV) file. This old PHP application can't be modified so there is no way to modernize it without risks. You will use CDC on the SQL server database where the orders are stored and the connector deployed on the Kafka Connect cluster using AMQ streams to emit data change events.

image::images/debezium-complete-cdc.png[Data replication - CDC]

In the second part of this tutorial, you will use the new Apache Camel Kafka Connectors to easily plug the events generated by debezium into an Elasticsearch index. The orders then will be indexed and stored for further retrieval.

image::images/debezium-complete-es.png[Data replication - Elasticsearch]

[type=walkthroughResource,serviceName=openshift]
.Red Hat OpenShift
****
* link:{openshift-console-url}[Console, window="_blank"]
****
// <-- END OF SOLUTION PATTERN GENERAL INFO -->

// <-- START OF SOLUTION PATTERN TASKS -->
[time=5]
[id='loading-orders']
== Loading Orders

[type=taskResource]
.Useful Resources
****
* link:https://raw.githubusercontent.com/RedHatWorkshops/dayinthelife-streaming/master/support/module-1/single-order.csv[Single Order File Download, window="_blank"]
* link:https://www-php-app-debezium-complete-demo.{openshift-app-host}[Legacy System Application, window="_blank"]
****

It’s now time to test the Change Data Capture integration.

- We will first load the CSV data into MSSQL using a PHP app
- Debezium will populate data from MSSQL into the Kafka topic

image::images/debezium-complete-cdc.png[Data replication - CDC]

=== Loading Orders File

Debezium’s SQL Server Connector can monitor and record the row-level changes in the schemas of a SQL Server database.

The functionality of the connector is based upon the change data capture feature provided by SQL Server Standard (since SQL Server 2016 SP1) or Enterprise edition.

. Download to your local system the link:https://raw.githubusercontent.com/RedHatWorkshops/dayinthelife-streaming/master/support/module-1/single-order.csv[Single Order File, window="_blank"].

. Navigate to the legacy PHP link:https://www-php-app-debezium-complete-demo.{openshift-app-host}[Enterprise System, window="_blank"] in a new browser tab.
+
image:images/www-main.png[Main Application]

. Click on the *Choose File* button to load the _orders_ file.
+
image:images/www-choose-file.png[Choose File]

. Load the _CSV_ file you just downloaded. Click on the *Open* button.
+
image:images/www-orders-file.png[Orders File]

. Now, click the *Load File* button. This will load the file orders and insert them in the database.
+
image:images/www-load-file.png[Load File]

.  Wait a few seconds for the information to load. If this is successfully completed, you will see the CSV data on the *Imported Orders* page.
+
image::images/www-imported-orders.png[Imported Orders]

[type=verification]
Were you able to get the orders list screen?

[type=verificationFail]
{standard-fail-text}

// <-- END OF SOLUTION PATTERN GENERAL INFO -->

// <-- START OF SOLUTION PATTERN TASKS -->
[time=5]
[id='check-database-orders']
== Check Database Records

After loading the file into the system, the orders data  should be stored in the SQL Server database. Let's check to make sure that's the case.

. Open a new browser tab and go to the link:{openshift-console-url}[topology view, window="_blank"]
+
[NOTE]
====
If you haven't done so, change from the Administrator to the Developer view on the top left menu.
====

. Click on the database deployment to open the overview page. Then click on the *Resources* tab and next the _Pod_ name.
+
image:images/openshift-db-overview.png[Access Pod]

. Now, click on the *Terminal* tab to access the pod's shell.
+
image:images/openshift-db-terminal.png[Pod Terminal]

. Run the following command to access the database:
+
[source,bash,subs="attributes+"]
----
/opt/mssql-tools/bin/sqlcmd -S mssql-server-linux -U sa -P Password! -d InternationalDB -Q "select * from dbo.Orders"
----

. Check the results, should look like this:
+
image::images/openshift-sqlcommand.png[Query]

[type=verification]
Is there a record in the database?

[type=verificationFail]
{standard-fail-text}

// <-- END OF SOLUTION PATTERN GENERAL INFO -->

// <-- START OF SOLUTION PATTERN TASKS -->
[time=5]
[id='check-kafka-topics']
== Check Apache Kafka

We checked that the database had the orders data. Is now time to check that the Change Data Capture connector was able to query the transaction log and generate the Apache Kafka events to the cluster.

=== Check the Kafka Topics

. Validate that the *KafkaTopics* were created.
+
--
- Expand the *More* menu on the left menu bar.
- Click on *Search* to load the _resources_ search page.
- Click on the _Resources_ field and type `topic`.
- Finally select *KafkaTopic*.

image::images/openshift-search-kafkatopics.png[Search KafkaTopics]

[TIP]
====
This an alternative way to search for resources in the OpenShift developer console.
====
--

. This will display the topics on the _Kafka cluster_. You should be able to locate your `debezium-cluster*` as well as the `mssql-server-linux.dbo.orders--*` _KafkaTopics_.
+
--
image::images/openshift-cdc-topics.png[CDC KafkaTopics]

[NOTE]
====
Don't worry for that awful long name! The GUID generated is only for the OpenShift custom resource management, the *actual* _topic name_ will be something more friendly.
====
--

[type=verification]
Were you able to see that your Change Data Capture connector has access to Kafka?

[type=verificationFail]
{standard-fail-text}

=== Inspect Kafka records

Time to check what information is flowing into Apache Kafka.

. Go back to the link:{openshift-console-url}[topology view, window="_blank"]
+
[NOTE]
====
If you haven't done so, change from the Administrator to the Developer view on the top left menu.
====

. Click on the `tooling` pod to open the right panel and then click on the pod name.
+
image::images/tooling-topology.png[Tooling, role="integr8ly-img-responsive"]

. Click on the `terminal` tab to get access to the working terminal.
+
image::images/tooling-terminal.png[Tooling terminal, role="integr8ly-img-responsive"]

. Issue the following command to read the _events_ in the `orders` topic:
+
[source,bash,subs="attributes+"]
----
kafkacat -b demo-kafka-bootstrap:9092 -t mssql-server-linux.dbo.Orders -e | jq .
----

. You should see the json output of the messages. Something like this:
+
----
% Auto-selecting Consumer mode (use -P or -C to override)
% Reached end of topic mssql-server-linux.dbo.Orders [0] at offset 1: exiting
{
  "OrderId": 1,
  "OrderType": "E",
  "OrderItemName": "Lime",
  "Quantity": 100,
  "Price": "3.69",
  "ShipmentAddress": "541-428 Nulla Avenue",
  "ZipCode": "4286",
  "OrderUser": "demo",
  "__op": "c",
  "__table": "Orders"
}
----

[IMPORTANT]
====
This is a reduced version of the debezium record structure. In this case we are using some of the Debezium embedded Single Message Transformation (SMTs) to _extract_ only the information of the *after* payload section and to add the required headers to detect from which table (`pass:[__table]`) is this record coming as well as the operation type (`pass:[__op]`). 

You can take a look at the *KafkaConnector* configuration to check how the SMTs are being used.
====

[type=verification]
Were you able to see the debezium record with the same information from the database insert?

[type=verificationFail]
{standard-fail-text}
// <-- END OF SOLUTION PATTERN GENERAL INFO -->

// <-- START OF SOLUTION PATTERN TASKS -->
[time=5]
[id="send-events-out"]
== Sending events out of Kafka to Elasticsearch

The link:https://camel.apache.org/[Apache Camel] community introduced recently a new subproject in the ecosystem: link:https://camel.apache.org/camel-kafka-connector/latest/[Camel Kafka Connector]. The main idea behind the project is reusing the Camel components’ flexibility in a simple way, through a configuration file mixing Kafka Connect configuration and Camel route definitions and options.

The Camel Kafka Connector allows you to use all Camel components as Kafka Connect connectors, which as result expands Kafka Connect compatibility to include all Camel components to be used in Kafka ecosystem.

image::images/debezium-complete-es.png[Data replication - Elasticsearch]

One of the main reasons to generate Kafka events from the Orders SQL server database is being able to share the information with other systems. In this case, the order fullfillment team requires to search the orders to find _business critical_ information. 

=== Review the Apache Camel Elasticsearch connector

. Go back to the link:{openshift-console-url}[topology view, window="_blank"]
+
[NOTE]
====
If you haven't done so, change from the Administrator to the Developer view on the top left menu.
====

. Review the *KafkaConnector* Custom Resource (CR).
+
--
- Expand the *More* menu on the left menu bar.
- Click on *Search* to load the _resources_ search page.
- Click on the _Resources_ field and type `kafka`.
- Finally select *KafkaConnector*.

image::images/openshift-search-kafkaconnectors.png[Search KafkaConnectors]

[IMPORTANT]
====
Double check that you are selecting *KafkaConnector* and not _KafkaConnect_. Notice the *or* at the end.
====
--

. Click on the `camel-connector`
+
image::images/openshift-camel-connector.png[Camel Connector]

. Click on the _YAML_ tab to access the connector CR configuration.
+
image::images/openshift-connector-detail.png[Connector Detail]

. You should notice the CR *spec:* containing the following configuration:
+
----
  class: >-
    org.apache.camel.kafkaconnector.elasticsearchrest.CamelElasticsearchrestSinkConnector
  config:
    camel.sink.endpoint.hostAddresses: 'elasticsearch:9200'
    camel.sink.endpoint.indexName: orders
    camel.sink.endpoint.operation: Index
    camel.sink.path.clusterName: elasticsearch
    key.converter: org.apache.kafka.connect.storage.StringConverter
    topics: mssql-server-linux.dbo.Orders
    value.converter: org.apache.kafka.connect.storage.StringConverter
  tasksMax: 1
----

. Check the configuration. It includes the required camel lines to make the component work adding endpoint fields like the `hostname`, `indexName` and `operation`. In our case, we will index all the _orders_ coming from the database.

. Scroll down to check the status of the Connector. It should look like this:
+
----
status:
  conditions:
    - lastTransitionTime: '2020-07-30T17:03:55.382Z'
      status: 'True'
      type: Ready
  connectorStatus:
    connector:
      state: RUNNING
      worker_id: '10.131.0.32:8083'
    name: camel-connector
    tasks:
      - id: 0
        state: RUNNING
        worker_id: '10.131.0.32:8083'
    type: sink
  observedGeneration: 1
----

. This section of the *KafkaConnector* resource gives us information on the state of the connector. You can also get this information by querying the REST API from the Kafka Connet cluster. If everything is fine, the connector should have 1 task and it should be in `state: RUNNING`.

[type=verification]
Is the connector in the correct state?

[type=verificationFail]
{standard-fail-text}

=== Review the connector log

. Go back to the link:{openshift-console-url}[topology view, window="_blank"]

. Click on the `camel-connect` circle under _strimzi-camel_ cluster. Then click on *resources* and finally on the *pod* name to access the running Kafka Connect container.
+
image::images/openshift-camel-connect.png[Kafka Connect Camel]

. When in the pod details, click on the *Logs* tab to access the container log.
+
image::images/openshift-camel-log.png[Kafka Connect Camel]

. You should be able to see that the Apache Camel route started successfully. Search for some lines similar to these:
+
----
2020-07-30 17:03:55,486 INFO Setting initial properties in Camel context: [{connector.class=org.apache.camel.kafkaconnector.elasticsearchrest.CamelElasticsearchrestSinkConnector, camel.sink.endpoint.operation=Index, camel.sink.path.clusterName=elasticsearch, camel.sink.component=elasticsearch-rest, task.class=org.apache.camel.kafkaconnector.elasticsearchrest.CamelElasticsearchrestSinkTask, topics=mssql-server-linux.dbo.Orders, tasks.max=1, camel.sink.endpoint.indexName=orders, camel.sink.endpoint.hostAddresses=elasticsearch:9200, name=camel-connector, value.converter=org.apache.kafka.connect.storage.StringConverter, key.converter=org.apache.kafka.connect.storage.StringConverter}] (org.apache.camel.kafkaconnector.utils.CamelMainSupport) [task-thread-camel-connector-0]
2020-07-30 17:03:55,506 INFO Creating Camel route from(direct:start).to(elasticsearch-rest:elasticsearch?hostAddresses=elasticsearch:9200&indexName=orders&operation=Index) (org.apache.camel.kafkaconnector.utils.CamelMainSupport) [task-thread-camel-connector-0]
2020-07-30 17:03:55,520 INFO Starting CamelContext (org.apache.camel.kafkaconnector.utils.CamelMainSupport) [task-thread-camel-connector-0]
2020-07-30 17:03:55,523 INFO Using properties from: classpath:application.properties;optional=true (org.apache.camel.main.BaseMainSupport) [pool-3-thread-1]
2020-07-30 17:03:55,538 INFO No additional Camel XML routes discovered from: classpath:camel/*.xml (org.apache.camel.main.DefaultRoutesCollector) [pool-3-thread-1]
2020-07-30 17:03:55,539 INFO No additional Camel XML rests discovered from: classpath:camel-rest/*.xml (org.apache.camel.main.DefaultRoutesCollector) [pool-3-thread-1]
2020-07-30 17:03:55,613 INFO Apache Camel 3.3.0 (CamelContext: camel-1) is starting (org.apache.camel.impl.engine.AbstractCamelContext) [pool-3-thread-1]
2020-07-30 17:03:55,614 INFO StreamCaching is not in use. If using streams then its recommended to enable stream caching. See more details at http://camel.apache.org/stream-caching.html (org.apache.camel.impl.engine.AbstractCamelContext) [pool-3-thread-1]
2020-07-30 17:03:55,620 INFO Connecting to the ElasticSearch cluster: elasticsearch (org.apache.camel.component.elasticsearch.ElasticsearchProducer) [pool-3-thread-1]
2020-07-30 17:03:55,759 INFO Route: route1 started and consuming from: direct://start (org.apache.camel.impl.engine.AbstractCamelContext) [pool-3-thread-1]
2020-07-30 17:03:55,760 INFO Total 1 routes, of which 1 are started (org.apache.camel.impl.engine.AbstractCamelContext) [pool-3-thread-1]
2020-07-30 17:03:55,760 INFO Apache Camel 3.3.0 (CamelContext: camel-1) started in 0.147 seconds (org.apache.camel.impl.engine.AbstractCamelContext) [pool-3-thread-1]
2020-07-30 17:03:55,761 INFO CamelContext started (org.apache.camel.kafkaconnector.utils.CamelMainSupport) [task-thread-camel-connector-0]
2020-07-30 17:03:55,761 INFO CamelSinkTask connector task started (org.apache.camel.kafkaconnector.CamelSinkTask) [task-thread-camel-connector-0]
----

[type=verification]
Did the connector started successfully?

[type=verificationFail]
{standard-fail-text}
// <-- END OF SOLUTION PATTERN TASKS -->

// <-- START OF SOLUTION PATTERN TASKS -->
[time=5]
[id='index-result']
== Check the indexed data on Elasticsearch

Now that we verified that the Apache Camel Kafka Connector is running in the Kafka Connect cluster is time to take a look at the Eleasticsearch REST API to check if our data was successfully indexed.

[type=taskResource]
.Useful Resources
****
* link:https://raw.githubusercontent.com/RedHatWorkshops/dayinthelife-streaming/master/support/module-1/earth-orders.csv[Multiple Orders File Download, window="_blank"]
* link:https://www-php-app-debezium-complete-demo.{openshift-app-host}[Legacy System Application, window="_blank"]
****

=== Review the index information

. In a new browser tab access the Elasticsearch index link:http://elastic-debezium-complete-demo.{openshift-app-host}/orders[REST endpoint].

. You should be able to retrieve a json objet with the information on the _mapping properties_ of our *orders*. It should look like the following (formatted for better view):
+
----
{
  "orders": {
    "aliases": {},
    "mappings": {
      "properties": {
        "OrderId": {
          "type": "long"
        },
        "OrderItemName": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "OrderType": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "OrderUser": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "Price": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "Quantity": {
          "type": "long"
        },
        "ShipmentAddress": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "ZipCode": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "__op": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "__table": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        }
      }
    },
    "settings": {
      "index": {
        "creation_date": "1596135798546",
        "number_of_shards": "1",
        "number_of_replicas": "1",
        "uuid": "1k0svQD0R6iOrGQBgK2LgA",
        "version": {
          "created": "7030099"
        },
        "provided_name": "orders"
      }
    }
  }
}
----

=== Do a index search

Time to execute a search on the index to retrieve all the records available. 

. Append the following to the URL path: `/_search` and hit enter.

. The result should appear in the browser window. It should look like this (formatted for better view):
+
----
{
  "took": 41,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 1,
    "hits": [
      {
        "_index": "orders",
        "_type": "_doc",
        "_id": "OcsboXMBOET50gSsdxHl",
        "_score": 1,
        "_source": {
          "OrderId": 1,
          "OrderType": "E",
          "OrderItemName": "Lime",
          "Quantity": 100,
          "Price": "3.69",
          "ShipmentAddress": "541-428 Nulla Avenue",
          "ZipCode": "4286",
          "OrderUser": "demo",
          "__op": "c",
          "__table": "Orders"
        }
      }
    ]
  }
}
----

[type=verification]
Is the information the same we entered in the app?

[type=verificationFail]
{standard-fail-text}

=== Load more orders

. Get back to the legacy link:https://www-php-app-debezium-complete-demo.{openshift-app-host}[Enterprise System, window="_blank"].
+
image:images/www-main.png[Main Application]

. Download to your local system the link:https://raw.githubusercontent.com/RedHatWorkshops/dayinthelife-streaming/master/support/module-1/earth-orders.csv[Mutiple Orders File, window="_blank"].

. Load the file into the application. This should load another *1000 orders* into the system.
+
image:images/www-load-multiple-file.png[Main Application]

. Wait a few seconds for the information to load. If this is successfully completed, you will see the CSV data on the *Imported Orders* page.
+
--
image:images/www-imported-multiple-orders.png[Main Application]

[TIP]
=====
You can follow the previous steps to check the data in the database and the kafka topic.
=====
--

[type=verification]
Is the information the same we entered in the app?

[type=verificationFail]
{standard-fail-text}

=== Search the Orders

. Get back to the browser tab where you have the `/_search` URL and refresh it. You should get now all the records. It should look like this (formatted for better view):
+
----
{
  "took": 412,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1001,
      "relation": "eq"
    },
    "max_score": 1,
    "hits": [
      {
        "_index": "orders",
        "_type": "_doc",
        "_id": "OcsboXMBOET50gSsdxHl",
        "_score": 1,
        "_source": {
          "OrderId": 1,
          "OrderType": "E",
          "OrderItemName": "Lime",
          "Quantity": 100,
          "Price": "3.69",
          "ShipmentAddress": "541-428 Nulla Avenue",
          "ZipCode": "4286",
          "OrderUser": "demo",
          "__op": "c",
          "__table": "Orders"
        }
      },
...
----

. Notice that this time we got a total of *1001* hits!

[type=verification]
Is the information the same we entered in the app?

[type=verificationFail]
{standard-fail-text}

=== Search the for Cherry

Let's filter by one of the fields. Let's search for all the orders that are `Cherry` flavored by searching the `OrderItemName`.

. Append the following query detail to the URL `?q=OrderItemName:Cherry` and hit enter.

. You should get the response for the query with the cherry flavored results. It should look like this (formatted for better view):
+
----
{
  "took": 26,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 34,
      "relation": "eq"
    },
    "max_score": 3.9419897,
    "hits": [
      {
        "_index": "orders",
        "_type": "_doc",
        "_id": "QctToXMBOET50gSsUxFM",
        "_score": 3.9419897,
        "_source": {
          "OrderId": 9,
          "OrderType": "E",
          "OrderItemName": "Cherry",
          "Quantity": 140,
          "Price": "4.42",
          "ShipmentAddress": "Ap #781-1741 Sem. St.",
          "ZipCode": "130336",
          "OrderUser": "demo",
          "__op": "c",
          "__table": "Orders"
        }
      },
...
----

As you can see now we just have *34* orders for Cherry. You can try to search for other fields on your own.

Congratulations you successfully replicated some data from your legacy application into an indexing service using Red Hat Integration with Debezium, Apache Kafka and Apache Camel!

[type=verification]
Did you get the same results?

[type=verificationFail]
{standard-fail-text}

// <-- END OF SOLUTION PATTERN TASKS -->

// <-- START OF SOLUTION PATTERN TASKS -->
[time=5]
[id='summary']
== Summary

After completing the tutorial, consider the following next steps:

- Explore the tutorial further.
+
Use the MySQL command line client to add, modify, and remove rows in the database tables, and see the effect on the topics. Keep in mind that you cannot remove a row that is referenced by a foreign key.

- Plan a Debezium deployment.
+
====
You can install Debezium in OpenShift or on Red Hat Enterprise Linux. For more information, see the following:

- link:https://access.redhat.com/documentation/en-us/red_hat_integration/2020-Q2/html-single/installing_change_data_capture_on_openshift/[Installing Debezium on OpenShift, window="_blank"]
- link:https://access.redhat.com/documentation/en-us/red_hat_integration/2020-Q2/html-single/installing_change_data_capture_on_rhel/[Installing Debezium on RHEL, window="_blank"]
====

// <-- END OF SOLUTION PATTERN TASKS -->